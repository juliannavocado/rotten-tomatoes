{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahvlstTQEh0w"
   },
   "source": [
    "# Juliann Negron Homework 2 - Rotten Tomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN9AI-hZEh0x"
   },
   "source": [
    "Your script should begin by defining two variables (after importing libraries, etc)\n",
    "movie        a string variable indicating the movie for which reviews will be parsed\n",
    "pageNum  the number of review pages to parse\n",
    "\n",
    "For example, to parse the first 3 pages of “Gangs of New York” reviews, set movie = “gangs_of_new_york” and pageNum = 3. Your code should then request the three pages below, and parse them.\n",
    "https://rottentomatoes.com/m/gangs_of_new_york/reviews?page=1 \n",
    "https://rottentomatoes.com/m/gangs_of_new_york/reviews?page=2 \n",
    "https://rottentomatoes.com/m/gangs_of_new_york/reviews?page=3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "8rD-35pCEh0x"
   },
   "outputs": [],
   "source": [
    "# 1. importing useful libraries\n",
    "\n",
    "import requests # to get the website\n",
    "import time     # to force our code to wait a little before re-trying to grab a webpage\n",
    "import re       # to grab the exact element we need\n",
    "from bs4 import BeautifulSoup # to grab the html elements we need\n",
    "\n",
    "# Import pandas to read in data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import models and evaluation functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "#from sklearn import cross_validation\n",
    "#from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import vectorizers to turn text into numeric\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import plotting\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "5PF4S6J8Eh03"
   },
   "outputs": [],
   "source": [
    "# Defining a list of 50 movies and page number variable\n",
    "movie_list = ['hamilton_2020', 'cats_2019', 'mamma_mia', 'into_the_woods_2014', 'annie_1981', 'les_miserables_2012', 'sweeney_todd_the_demon_barber_of_fleet_street_2007', 'hairspray', \"1152276-rent\", 'dreamgirls', \n",
    "              'phantom_of_the_opera', 'chicago', 'moulin_rouge_2001', \"1079818-anastasia\", 'prince_of_egypt', 'newsies', \"1073037-hunchback_of_notre_dame\", 'frozen_2013', 'the_last_5_years', \"1012514-little_shop_of_horrors\", \n",
    "              'grease', 'jersey_boys', 'evita', 'funny_girl', 'oliver', 'joseph_and_the_amazing_technicolor_dreamcoat', \"1011605_king_and_i\", 'west_side_story', 'guys_and_dolls', 'south_pacific', \n",
    "              'american_in_paris', 'singin_in_the_rain', 'oklahoma', 'brigadoon', 'kiss-me-kate1953', 'gigi', 'the_pajama_game', \"1005152-damn_yankees\", 'sound_of_music', 'beauty_and_the_beast_1991', \n",
    "              'aladdin', 'mary_poppins', 'the_lion_king', 'fiddler_on_the_roof', 'hair', \"1003339-bye_bye_birdie\", \"1014453-music_man\", 'once', \"1205483_nine\", 'xanadu']\n",
    "PageNum = 4\n",
    "\n",
    "# Starting with an empty list that we will add to in the loop\n",
    "movie_review_data = []\n",
    "\n",
    "# Creating for loop to loop through every entry in my movie list\n",
    "\n",
    "for m in movie_list:\n",
    "\n",
    "    # Starting with this url, will add to it later\n",
    "    first_url = 'https://www.rottentomatoes.com/m/' + str(m) + '/reviews'\n",
    "    \n",
    "    # Creating a for loop to scape movie data from number of pages provided\n",
    "    for i in range(1, PageNum + 1):\n",
    "\n",
    "        # First page is different from the rest, so I made an if statement to give it a custom url\n",
    "        if i == 1:\n",
    "            current_page = first_url\n",
    "            \n",
    "        # All other pages have this layout for the url\n",
    "        else:\n",
    "            current_page = first_url + '?page='+str(i)\n",
    "            \n",
    "        # try to scrape times\n",
    "        for k in range(5): \n",
    "            try:\n",
    "                # get url content\n",
    "                response = requests.get(current_page,headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36', })\n",
    "                # get the html\n",
    "                html=response.content\n",
    "                # if we successuflly got the file, break the loop\n",
    "                break \n",
    "            # requests.get() threw an exception, i.e., the attempt to get the response failed\n",
    "            except:\n",
    "                print ('failed attempt #',k)\n",
    "                # wait 2 secs before trying again\n",
    "                time.sleep(2)\n",
    "\n",
    "        if not html:\n",
    "            # couldnt get the page, ignore\n",
    "            print('could not get page #', i)\n",
    "            continue \n",
    "        \n",
    "        # Making the HTML file more readable\n",
    "        current_content = BeautifulSoup(html.decode('ascii', 'ignore'), 'lxml')\n",
    "        \n",
    "        # keep only <div> tags whose class contains the 'review_table_row' substring\n",
    "        review_data = current_content.findAll('div', {'class':re.compile('review_table_row')})\n",
    "    \n",
    "        # For loop to save the review text and the rating into variables to add to list at the end\n",
    "        for k in review_data:\n",
    "            \n",
    "            # Rating and text variables start as NA and stay as NA if variables are not found\n",
    "            rating = 'NA'\n",
    "            text = 'NA'\n",
    "            \n",
    "            # 1. If there is a rating, get it\n",
    "            rate_data = k.find('div', {'class': re.compile('review_icon')})\n",
    "            rating_text = str(rate_data.attrs['class'])\n",
    "            \n",
    "            # Inputting good or \"fresh\" movies as 1, \"rotten\" movies as 0\n",
    "            if rating_text: \n",
    "                if 'fresh' in rating_text:\n",
    "                    rating = 1   \n",
    "                else:\n",
    "                    rating = 0\n",
    "        \n",
    "            # 2. If there is review text, get it  \n",
    "            text_data = k.find('div',{'class':'the_review'})\n",
    "                \n",
    "            if text_data: \n",
    "                text = text_data.text.strip() \n",
    "            \n",
    "            # Appending to movie list as a list (creating a list of lists)\n",
    "            movie_review_data.append([rating, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "-8l1gE8YEh07"
   },
   "outputs": [],
   "source": [
    "# Creating column names to build data frame\n",
    "col_names = ('rating', 'text')\n",
    "\n",
    "# Converting list of lists into data frame\n",
    "movie_df = pd.DataFrame(movie_review_data, columns=col_names)\n",
    "\n",
    "# Replacing all empty values in my data frame with \"NaN\"\n",
    "nan_value = float(\"NaN\") \n",
    "movie_df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "# Removing all entries without review text\n",
    "movie_data = movie_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaoWRWnTEh1E"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmFULSVVEh1F",
    "outputId": "7726b912-cd34-447f-c4a5-29008f7ecb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating                                               text\n",
      "0          1  There's a line where Hamitlon says \"America, y...\n",
      "1          1  Hamilton is above all an exploration of the co...\n",
      "2          1  Rarely does an uber-hyped pop culture phenomen...\n",
      "3          1  The film pulses with energy and life - and yes...\n",
      "4          1  The vast majority of this is accurate, which i...\n",
      "...      ...                                                ...\n",
      "2721       0  Xanadu is a mushy and limp musical fantasy, so...\n",
      "2722       0  Xanadu doesn't lend itself to quick or easy ch...\n",
      "2724       1  Great tunes fill this flavorful bubble gum movie.\n",
      "2728       0   campy joyful fun, with a wee bit too much cheese\n",
      "2733       1  There's never been a movie quite like this, an...\n",
      "\n",
      "[2492 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#len(movie_df)\n",
    "# Length of data frame used to be 2734 rows, now 2492 rows\n",
    "print(movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU6DN81tEh1I"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "U7hk9-jhEh1J"
   },
   "outputs": [],
   "source": [
    "# Part II: Working classifier\n",
    "\n",
    "# Separating columns into two vectors, using X to predict Y\n",
    "X_text = movie_data['text']\n",
    "Y_rating = movie_data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeQ9CYALjDEm",
    "outputId": "74ab7a22-80c3-4379-8f19-cd29e5481ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our classifier is 0.724\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as counted features\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "count_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = count_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation AUCs\n",
    "accs = cross_val_score(logistic_regression, X, Y_rating, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print(\"Accuracy of our classifier is \" + str(round(np.mean(accs), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_e0-QTBoj4Y1",
    "outputId": "1eb04411-ffca-4c14-a931-ef5d7a772bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our classifier is 0.739\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF\n",
    "# Create a vectorizer that will track text as binary features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation AUCs\n",
    "aucs = cross_val_score(logistic_regression, X, Y_rating, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print(\"Accuracy of our classifier is \" + str(round(np.mean(aucs), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLoxDxIDo2lE"
   },
   "source": [
    "It appears that the TF-IDF method is more accurate than using full counts instead of a binary representation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "JuliannNegronHomework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
